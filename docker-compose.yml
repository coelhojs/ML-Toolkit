# A docker-compose must always start by the version tag.
# We use "3" because it's the last version at this time.
version: "3"

# You should know that docker-composes works with services.
# 1 service = 1 container.
# For example, a service maybe, a server, a client, a database...
# We use the keyword 'services' to start to create services.
services:
  # As we said at the beginning, we want to create: a server and a client.
  # That is two services.

  # First service (container): the server.
  # Here you are free to choose the keyword.
  # It will allow you to define what the service corresponds to.
  # We use the keyword 'server' for the server.
  server:
    # The keyword "build" will allow you to define
    # the path to the Dockerfile to use to create the image
    # that will allow you to execute the service.
    # Here 'server/' corresponds to the path to the server folder
    # that contains the Dockerfile to use.
    build: API/

    # The command to execute once the image is created.
    # The following command will execute "python ./server.py".
    # command: python3 ./server.py

    # Remember that we defined in'server/server.py' 1234 as port.
    # If we want to access the server from our computer (outside the container),
    # we must share the content port with our computer's port.
    # To do this, the keyword 'ports' will help us.
    # Its syntax is as follows: [port we want on our machine]:[port we want to retrieve in the container]
    # In our case, we want to use port 1234 on our machine and
    # retrieve port 1234 from the container (because it is on this port that
    # we broadcast the server).
    ports:
      - 5000:5000

  # Second service (container): the client.
  # We use the keyword 'client' for the server.
  ml-inference:
    # Here 'client/ corresponds to the path to the client folder
    # that contains the Dockerfile to use.
    build: Inference-Caller/

    # The command to execute once the image is created.
    # The following command will execute "python ./client.py".
    # command: python ./client.py

    # The keyword 'network_mode' is used to define the network type.
    # Here we define that the container can access to the 'localhost' of the computer.
    network_mode: host
    ports: 
        - 8502:8502
    # The keyword'depends_on' allows you to define whether the service
    # should wait until other services are ready before launching.
    # Here, we want the 'client' service to wait until the 'server' service is ready.
    depends_on:
      - server
  tf-serving:
    command: --model_config_file=/models/models.config
    depends_on:
        - server
    image: tensorflow/serving
    network_mode: host
    ports:
        - 8501:8501
    volumes:
        -  /home/cristiano/development/ML-Toolkit/Tensorflow-Serving/models:/models/
    #docker run -p 8501:8501 --mount type=bind,source=C:/Machine-Learning-Models-Server/models/,
    #target=/models/ --mount type=bind,source=C:/Machine-Learning-Models-Server/models/models.config,
    #target=/models/models.config -t tensorflow/serving --model_config_file=/models/models.config